{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Synaptic metaplasticity in Binarized Neural Networks\n",
    "\n",
    "Based on : https://github.com/Laborieux-Axel/SynapticMetaplasticityBNN\n",
    "\n",
    "With this notebook, you can do this experience:\n",
    "- permuted MNISTs\n",
    "- MNIST then FashionMNIST"
   ],
   "metadata": {
    "id": "zf_8pAgvtmRv"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import matplotlib\n",
    "import copy\n",
    "\n",
    "from collections import OrderedDict\n",
    "from datetime import datetime\n",
    "from PIL import Image"
   ],
   "metadata": {
    "id": "NEA9x9zAMcef"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "torch.cuda.is_available()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zzuiLgVCQrVq",
    "outputId": "8f9edbe2-ae67-493e-c313-b96961e31f34"
   },
   "execution_count": 8,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "seed = 42\n",
    "# Hyperparameters\n",
    "device = \"cuda\"\n",
    "# m = 1.5\n",
    "m = 1.35\n",
    "lr = 0.005\n",
    "epochs = 40\n",
    "gamma = 1.0\n",
    "decay = 1e-7\n",
    "net = \"bnn\"\n",
    "archi = [784, 4096, 4096, 10]\n",
    "scenario = \"task\"\n",
    "\n",
    "# For MNIST-FMNIST\n",
    "# task_sequence = [\"MNIST\", \"FMNIST\"]\n",
    "# epochs = 50\n",
    "# m = 1.5\n",
    "\n",
    "# For pMNIST\n",
    "task_sequence = [\"pMNIST\", \"pMNIST\", \"pMNIST\"]\n",
    "epochs = 40\n",
    "m = 1.35\n",
    "\n",
    "init = \"uniform\"\n",
    "init_width = 0.1\n",
    "norm = \"bn\"\n",
    "rnd_consolidation = False\n",
    "bin_path = False\n",
    "# EWC\n",
    "ewc = False\n",
    "ewc_lambda = 0.0\n",
    "# Synaptic intelligence\n",
    "si = False\n",
    "si_lambda = 0.0\n",
    "# beaker\n",
    "beaker = False\n",
    "n_bk = 4\n",
    "ratios = [1e-2, 1e-3, 1e-4, 1e-5]\n",
    "fb = 5e-3\n",
    "areas = [1, 2, 4, 8]"
   ],
   "metadata": {
    "id": "ed3eL8xTMcej"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "cellView": "form",
    "id": "qn83N2w3Mcel"
   },
   "outputs": [],
   "source": [
    "# @title data_utils.py\n",
    "transform = torchvision.transforms.Compose(\n",
    "    [\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize(mean=(0.0,), std=(1.0,)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "mnist_dset_train = torchvision.datasets.MNIST(\n",
    "    \"./mnist_pytorch\",\n",
    "    train=True,\n",
    "    transform=transform,\n",
    "    target_transform=None,\n",
    "    download=True,\n",
    ")\n",
    "mnist_train_loader = torch.utils.data.DataLoader(\n",
    "    mnist_dset_train, batch_size=100, shuffle=True, num_workers=0\n",
    ")\n",
    "\n",
    "mnist_dset_test = torchvision.datasets.MNIST(\n",
    "    \"./mnist_pytorch\",\n",
    "    train=False,\n",
    "    transform=transform,\n",
    "    target_transform=None,\n",
    "    download=True,\n",
    ")\n",
    "mnist_test_loader = torch.utils.data.DataLoader(\n",
    "    mnist_dset_test, batch_size=100, shuffle=False, num_workers=0\n",
    ")\n",
    "\n",
    "fmnist_dset_train = torchvision.datasets.FashionMNIST(\n",
    "    \"./fmnist_pytorch\",\n",
    "    train=True,\n",
    "    transform=transform,\n",
    "    target_transform=None,\n",
    "    download=True,\n",
    ")\n",
    "fashion_mnist_train_loader = torch.utils.data.DataLoader(\n",
    "    fmnist_dset_train, batch_size=100, shuffle=True, num_workers=0\n",
    ")\n",
    "\n",
    "fmnist_dset_test = torchvision.datasets.FashionMNIST(\n",
    "    \"./fmnist_pytorch\",\n",
    "    train=False,\n",
    "    transform=transform,\n",
    "    target_transform=None,\n",
    "    download=True,\n",
    ")\n",
    "fashion_mnist_test_loader = torch.utils.data.DataLoader(\n",
    "    fmnist_dset_test, batch_size=100, shuffle=False, num_workers=0\n",
    ")\n",
    "\n",
    "\n",
    "def create_permuted_loaders(task):\n",
    "\n",
    "    permut = torch.from_numpy(np.random.permutation(784))\n",
    "\n",
    "    transform = torchvision.transforms.Compose(\n",
    "        [\n",
    "            torchvision.transforms.ToTensor(),\n",
    "            torchvision.transforms.Lambda(lambda x: x.view(-1)[permut].view(1, 28, 28)),\n",
    "            torchvision.transforms.Normalize(mean=(0.0,), std=(1.0,)),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    if task == \"MNIST\":\n",
    "        dset_train = torchvision.datasets.MNIST(\n",
    "            \"./mnist_pytorch\",\n",
    "            train=True,\n",
    "            transform=transform,\n",
    "            target_transform=None,\n",
    "            download=True,\n",
    "        )\n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            dset_train, batch_size=100, shuffle=True, num_workers=0\n",
    "        )\n",
    "\n",
    "        dset_test = torchvision.datasets.MNIST(\n",
    "            \"./mnist_pytorch\",\n",
    "            train=False,\n",
    "            transform=transform,\n",
    "            target_transform=None,\n",
    "            download=True,\n",
    "        )\n",
    "        test_loader = torch.utils.data.DataLoader(\n",
    "            dset_test, batch_size=100, shuffle=False, num_workers=0\n",
    "        )\n",
    "\n",
    "    elif task == \"FMNIST\":\n",
    "\n",
    "        dset_train = torchvision.datasets.FashionMNIST(\n",
    "            \"./fmnist_pytorch\",\n",
    "            train=True,\n",
    "            transform=transform,\n",
    "            target_transform=None,\n",
    "            download=True,\n",
    "        )\n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            dset_train, batch_size=100, shuffle=True, num_workers=0\n",
    "        )\n",
    "\n",
    "        dset_test = torchvision.datasets.FashionMNIST(\n",
    "            \"./fmnist_pytorch\",\n",
    "            train=False,\n",
    "            transform=transform,\n",
    "            target_transform=None,\n",
    "            download=True,\n",
    "        )\n",
    "        test_loader = torch.utils.data.DataLoader(\n",
    "            dset_test, batch_size=100, shuffle=False, num_workers=0\n",
    "        )\n",
    "\n",
    "    return train_loader, test_loader, dset_train\n",
    "\n",
    "\n",
    "class DatasetProcessing(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, target, transform=None):\n",
    "        self.transform = transform\n",
    "        self.data = data.astype(np.float32)[:, :, None]\n",
    "        self.target = torch.from_numpy(target).long()\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.transform is not None:\n",
    "            return self.transform(self.data[index]), self.target[index]\n",
    "        else:\n",
    "            return self.data[index], self.target[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(list(self.data))\n",
    "\n",
    "\n",
    "def process_features(X_train, X_test, mode):\n",
    "    if mode == \"cutoff\":\n",
    "        cutoff = 8\n",
    "        threshold_train = np.zeros((np.shape(X_train)[0], 1))\n",
    "        threshold_test = np.zeros((np.shape(X_test)[0], 1))\n",
    "        for i in range(np.shape(X_train)[0]):\n",
    "            threshold_train[i, 0] = np.unique(X_train[i, :])[-cutoff]\n",
    "        for i in range(np.shape(X_test)[0]):\n",
    "            threshold_test[i, 0] = np.unique(X_test[i, :])[-cutoff]\n",
    "        X_train = (np.sign(X_train - threshold_train + 1e-6) + 1.0) / 2\n",
    "        X_test = (np.sign(X_test - threshold_test + 1e-6) + 1.0) / 2\n",
    "    elif mode == \"mean_over_examples\":\n",
    "        X_train = (X_train - X_train.mean(axis=0, keepdims=True)) / X_train.var(\n",
    "            axis=0, keepdims=True\n",
    "        )  # ???\n",
    "        X_test = (X_test - X_test.mean(axis=0, keepdims=True)) / X_test.var(\n",
    "            axis=0, keepdims=True\n",
    "        )\n",
    "    elif mode == \"mean_over_examples_sign\":\n",
    "        X_train = (np.sign(X_train - X_train.mean(axis=0, keepdims=True)) + 1.0) / 2\n",
    "        X_test = (np.sign(X_test - X_test.mean(axis=0, keepdims=True)) + 1.0) / 2\n",
    "    elif mode == \"mean_over_pixels\":\n",
    "        X_train = (X_train - X_train.mean(axis=1, keepdims=True)) / X_train.var(\n",
    "            axis=1, keepdims=True\n",
    "        )  # Instance norm\n",
    "        X_test = (X_test - X_test.mean(axis=1, keepdims=True)) / X_test.var(\n",
    "            axis=1, keepdims=True\n",
    "        )\n",
    "    elif mode == \"mean_over_pixels_sign\":\n",
    "        X_train = (np.sign(X_train - X_train.mean(axis=1, keepdims=True)) + 1.0) / 2\n",
    "        X_test = (np.sign(X_test - X_test.mean(axis=1, keepdims=True)) + 1.0) / 2\n",
    "    elif mode == \"global_mean\":\n",
    "        X_train = (X_train - X_train.mean(keepdims=True)) / X_train.var(\n",
    "            keepdims=True\n",
    "        )  # Batch norm\n",
    "        X_test = (X_test - X_test.mean(keepdims=True)) / X_test.var(keepdims=True)\n",
    "    elif mode == \"rescale\":\n",
    "        X_train = X_train / X_train.max(axis=1, keepdims=True)\n",
    "        X_test = X_test / X_test.max(axis=1, keepdims=True)\n",
    "    return X_train, X_test\n",
    "\n",
    "\n",
    "def relabel(label):\n",
    "    label_map = [5, 6, 0, 1, 2, 3, 4, 7, 8, 9]\n",
    "    return label_map[label]\n",
    "\n",
    "\n",
    "vrelabel = np.vectorize(relabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# @title model_utils.py\n",
    "class SignActivation(\n",
    "    torch.autograd.Function\n",
    "):  # We define a sign activation with derivative equal to clip\n",
    "    @staticmethod\n",
    "    def forward(ctx, i):\n",
    "        result = i.sign()\n",
    "        ctx.save_for_backward(i)\n",
    "        return result\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        (i,) = ctx.saved_tensors\n",
    "        grad_i = grad_output.clone()\n",
    "        grad_i[i.abs() > 1.0] = 0\n",
    "        return grad_i\n",
    "\n",
    "\n",
    "def Binarize(tensor):\n",
    "    return tensor.sign()\n",
    "\n",
    "\n",
    "class BinarizeLinear(torch.nn.Linear):\n",
    "    def __init__(self, *kargs, **kwargs):\n",
    "        super(BinarizeLinear, self).__init__(*kargs, **kwargs)\n",
    "\n",
    "    def forward(self, input):\n",
    "\n",
    "        if input.size(1) != 784:\n",
    "            input.data = Binarize(input.data)\n",
    "        if not hasattr(self.weight, \"org\"):\n",
    "            self.weight.org = self.weight.data.clone()\n",
    "        self.weight.data = Binarize(self.weight.org)\n",
    "        out = torch.nn.functional.linear(input, self.weight)\n",
    "        if not self.bias is None:\n",
    "            self.bias.org = self.bias.data.clone()\n",
    "            out += self.bias.view(1, -1).expand_as(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class BinarizeConv2d(torch.nn.Conv2d):\n",
    "    def __init__(self, *kargs, **kwargs):\n",
    "        super(BinarizeConv2d, self).__init__(*kargs, **kwargs)\n",
    "\n",
    "    def forward(self, input):\n",
    "        if input.size(1) != 3:\n",
    "            input.data = Binarize(input.data)\n",
    "        if not hasattr(self.weight, \"org\"):\n",
    "            self.weight.org = self.weight.data.clone()\n",
    "        self.weight.data = Binarize(self.weight.org)\n",
    "\n",
    "        out = torch.nn.functional.conv2d(\n",
    "            input,\n",
    "            self.weight,\n",
    "            None,\n",
    "            self.stride,\n",
    "            self.padding,\n",
    "            self.dilation,\n",
    "            self.groups,\n",
    "        )\n",
    "\n",
    "        if not self.bias is None:\n",
    "            self.bias.org = self.bias.data.clone()\n",
    "            out += self.bias.view(1, -1, 1, 1).expand_as(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class BNN(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    MyNet can consist either of fc layers followed by batchnorm, fc weights being either float kind=\"classical_bn\"\n",
    "    or binarized kind=\"binary\", or fc layers with biases kind=\"classical_bias\". When BatchNorm is used the adtication function is\n",
    "    the sign function and when biases are used the activation function is Tanh\n",
    "    weights can be initialized to gaussian with init=\"gauss\" or uniform distribution with init=\"uniform\"\n",
    "    The width of the distribution is tuned with width\n",
    "    the only non specified argument is the list of neurons [input, hidden ... , output]\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, layers_dims, init=\"gauss\", width=0.01, norm=\"bn\"):\n",
    "        super(BNN, self).__init__()\n",
    "\n",
    "        self.hidden_layers = len(layers_dims) - 2\n",
    "        self.layers_dims = layers_dims\n",
    "        self.norm = norm\n",
    "\n",
    "        layer_list = []\n",
    "\n",
    "        for layer in range(self.hidden_layers + 1):\n",
    "            layer_list = layer_list + [\n",
    "                (\n",
    "                    (\"fc\" + str(layer + 1)),\n",
    "                    BinarizeLinear(\n",
    "                        layers_dims[layer], layers_dims[layer + 1], bias=False\n",
    "                    ),\n",
    "                )\n",
    "            ]\n",
    "            if norm == \"bn\":\n",
    "                layer_list = layer_list + [\n",
    "                    (\n",
    "                        (norm + str(layer + 1)),\n",
    "                        torch.nn.BatchNorm1d(\n",
    "                            layers_dims[layer + 1],\n",
    "                            affine=True,\n",
    "                            track_running_stats=True,\n",
    "                        ),\n",
    "                    )\n",
    "                ]\n",
    "            elif norm == \"in\":\n",
    "                layer_list = layer_list + [\n",
    "                    (\n",
    "                        (norm + str(layer + 1)),\n",
    "                        torch.nn.InstanceNorm1d(\n",
    "                            layers_dims[layer + 1],\n",
    "                            affine=False,\n",
    "                            track_running_stats=False,\n",
    "                        ),\n",
    "                    )\n",
    "                ]\n",
    "\n",
    "        self.layers = torch.nn.ModuleDict(OrderedDict(layer_list))\n",
    "\n",
    "        # weight init\n",
    "        for layer in range(self.hidden_layers + 1):\n",
    "            if init == \"gauss\":\n",
    "                torch.nn.init.normal_(\n",
    "                    self.layers[\"fc\" + str(layer + 1)].weight, mean=0, std=width\n",
    "                )\n",
    "            if init == \"uniform\":\n",
    "                torch.nn.init.uniform_(\n",
    "                    self.layers[\"fc\" + str(layer + 1)].weight, a=-width / 2, b=width / 2\n",
    "                )\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        size = self.layers_dims[0]\n",
    "        x = x.view(-1, size)\n",
    "\n",
    "        for layer in range(self.hidden_layers + 1):\n",
    "            x = self.layers[\"fc\" + str(layer + 1)](x)\n",
    "            # x = torch.nn.functional.dropout(x, p = 0.5, training = self.training)\n",
    "            if self.norm == \"in\":  # IN needs channel dim\n",
    "                x.unsqueeze_(1)\n",
    "            x = self.layers[self.norm + str(layer + 1)](x)\n",
    "            if self.norm == \"in\":  # Remove channel dim\n",
    "                x.squeeze_(1)\n",
    "            if layer != self.hidden_layers:\n",
    "                x = SignActivation.apply(x)\n",
    "        return x\n",
    "\n",
    "    def save_bn_states(self):\n",
    "        bn_states = []\n",
    "        if \"bn1\" in self.layers.keys():\n",
    "            for l in range(self.hidden_layers + 1):\n",
    "                bn = copy.deepcopy(self.layers[\"bn\" + str(l + 1)].state_dict())\n",
    "                bn_states.append(bn)\n",
    "        return bn_states\n",
    "\n",
    "    def load_bn_states(self, bn_states):\n",
    "        if \"bn1\" in self.layers.keys():\n",
    "            for l in range(self.hidden_layers + 1):\n",
    "                self.layers[\"bn\" + str(l + 1)].load_state_dict(bn_states[l])\n",
    "\n",
    "\n",
    "class ConvBNN(torch.nn.Module):\n",
    "    def __init__(self, init=\"gauss\", width=0.01, norm=\"bn\"):\n",
    "        super(ConvBNN, self).__init__()\n",
    "\n",
    "        self.norm = norm\n",
    "        self.hidden_layers = 2\n",
    "\n",
    "        layer_list = [\n",
    "            (\n",
    "                (\"cv1\"),\n",
    "                BinarizeConv2d(1, 32, kernel_size=5, padding=2, stride=2, bias=False),\n",
    "            ),  # out: (mb x 32 x 14 x 14)\n",
    "            ((norm + \"1\"), self.normalization(32, 2)),\n",
    "            (\n",
    "                (\"cv2\"),\n",
    "                BinarizeConv2d(32, 64, kernel_size=4, padding=2, stride=2, bias=False),\n",
    "            ),  # out ( mb x 64 x 8 x 8)\n",
    "            ((norm + \"2\"), self.normalization(64, 2)),\n",
    "            ((\"fc3\"), BinarizeLinear(64 * 64, 10, bias=False)),\n",
    "            ((norm + \"3\"), self.normalization(10, 1)),\n",
    "        ]\n",
    "\n",
    "        self.layers = torch.nn.ModuleDict(OrderedDict(layer_list))\n",
    "\n",
    "        for key in self.layers.keys():\n",
    "            if not (norm in key):\n",
    "                if init == \"gauss\":\n",
    "                    torch.nn.init.normal_(self.layers[key].weight, mean=0, std=width)\n",
    "                if init == \"uniform\":\n",
    "                    torch.nn.init.uniform_(\n",
    "                        self.layers[key].weight, a=-width / 2, b=width / 2\n",
    "                    )\n",
    "\n",
    "    def normalization(self, size, dim):\n",
    "        if self.norm == \"in\":\n",
    "            if dim == 2:\n",
    "                return torch.nn.InstanceNorm2d(\n",
    "                    size, affine=False, track_running_stats=False\n",
    "                )\n",
    "            else:\n",
    "                return torch.nn.InstanceNorm1d(\n",
    "                    size, affine=False, track_running_stats=False\n",
    "                )\n",
    "        elif self.norm == \"bn\":\n",
    "            if dim == 2:\n",
    "                return torch.nn.BatchNorm2d(size, affine=True, track_running_stats=True)\n",
    "            else:\n",
    "                return torch.nn.BatchNorm1d(size, affine=True, track_running_stats=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.layers[\"cv1\"](x)\n",
    "        if self.norm == \"in\":  # IN needs channel dim\n",
    "            x = self.layers[self.norm + \"1\"](x.unsqueeze_(1)).squeeze_(1)\n",
    "        else:\n",
    "            x = self.layers[self.norm + \"1\"](x)\n",
    "        x = SignActivation.apply(x)\n",
    "\n",
    "        x = self.layers[\"cv2\"](x)\n",
    "        if self.norm == \"in\":  # IN needs channel dim\n",
    "            x = self.layers[self.norm + \"2\"](x.unsqueeze_(1)).squeeze_(1)\n",
    "        else:\n",
    "            x = self.layers[self.norm + \"2\"](x)\n",
    "        x = SignActivation.apply(x)\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.layers[\"fc3\"](x)\n",
    "        if self.norm == \"in\":  # IN needs channel dim\n",
    "            x = self.layers[self.norm + \"3\"](x.unsqueeze_(1)).squeeze_(1)\n",
    "        else:\n",
    "            x = self.layers[self.norm + \"3\"](x)\n",
    "        return x\n",
    "\n",
    "    def save_bn_states(self):\n",
    "        bn_states = []\n",
    "        if \"bn1\" in self.layers.keys():\n",
    "            for l in range(self.hidden_layers + 1):\n",
    "                bn = copy.deepcopy(self.layers[\"bn\" + str(l + 1)].state_dict())\n",
    "                bn_states.append(bn)\n",
    "        return bn_states\n",
    "\n",
    "    def load_bn_states(self, bn_states):\n",
    "        if \"bn1\" in self.layers.keys():\n",
    "            for l in range(self.hidden_layers + 1):\n",
    "                self.layers[\"bn\" + str(l + 1)].load_state_dict(bn_states[l])\n",
    "\n",
    "\n",
    "class Adam_meta(torch.optim.Optimizer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        params,\n",
    "        lr=1e-3,\n",
    "        betas=(0.9, 0.999),\n",
    "        meta={},\n",
    "        eps=1e-8,\n",
    "        weight_decay=0,\n",
    "        amsgrad=False,\n",
    "    ):\n",
    "        if not 0.0 <= lr:\n",
    "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
    "        if not 0.0 <= eps:\n",
    "            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n",
    "        if not 0.0 <= betas[0] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n",
    "        if not 0.0 <= betas[1] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n",
    "        defaults = dict(\n",
    "            lr=lr,\n",
    "            betas=betas,\n",
    "            meta=meta,\n",
    "            eps=eps,\n",
    "            weight_decay=weight_decay,\n",
    "            amsgrad=amsgrad,\n",
    "        )\n",
    "        super(Adam_meta, self).__init__(params, defaults)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super(Adam_meta, self).__setstate__(state)\n",
    "        for group in self.param_groups:\n",
    "            group.setdefault(\"amsgrad\", False)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"Performs a single optimization step.\n",
    "\n",
    "        Arguments:\n",
    "            closure (callable, optional): A closure that reevaluates the model\n",
    "                and returns the loss.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError(\n",
    "                        \"Adam does not support sparse gradients, please consider SparseAdam instead\"\n",
    "                    )\n",
    "                amsgrad = group[\"amsgrad\"]\n",
    "\n",
    "                state = self.state[p]\n",
    "\n",
    "                # State initialization\n",
    "                if len(state) == 0:\n",
    "                    state[\"step\"] = 0\n",
    "                    # Exponential moving average of gradient values\n",
    "                    state[\"exp_avg\"] = torch.zeros_like(p.data)\n",
    "                    # Exponential moving average of squared gradient values\n",
    "                    state[\"exp_avg_sq\"] = torch.zeros_like(p.data)\n",
    "\n",
    "                    if amsgrad:\n",
    "                        # Maintains max of all exp. moving avg. of sq. grad. values\n",
    "                        state[\"max_exp_avg_sq\"] = torch.zeros_like(p.data)\n",
    "\n",
    "                exp_avg, exp_avg_sq = state[\"exp_avg\"], state[\"exp_avg_sq\"]\n",
    "\n",
    "                if amsgrad:\n",
    "                    max_exp_avg_sq = state[\"max_exp_avg_sq\"]\n",
    "                beta1, beta2 = group[\"betas\"]\n",
    "\n",
    "                state[\"step\"] += 1\n",
    "\n",
    "                if group[\"weight_decay\"] != 0:\n",
    "                    grad.add_(p.data, alpha=group[\"weight_decay\"])\n",
    "\n",
    "                # Decay the first and second moment running average coefficient\n",
    "                exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)\n",
    "                if amsgrad:\n",
    "                    # Maintains the maximum of all 2nd moment running avg. till now\n",
    "                    torch.max(max_exp_avg_sq, exp_avg_sq, out=max_exp_avg_sq)\n",
    "                    # Use the max. for normalizing running avg. of gradient\n",
    "                    denom = max_exp_avg_sq.sqrt().add_(group[\"eps\"])\n",
    "                else:\n",
    "                    denom = exp_avg_sq.sqrt().add_(group[\"eps\"])\n",
    "\n",
    "                bias_correction1 = 1 - beta1 ** state[\"step\"]\n",
    "                bias_correction2 = 1 - beta2 ** state[\"step\"]\n",
    "                step_size = group[\"lr\"] * math.sqrt(bias_correction2) / bias_correction1\n",
    "\n",
    "                binary_weight_before_update = torch.sign(p.data)\n",
    "                condition_consolidation = (\n",
    "                    torch.mul(binary_weight_before_update, exp_avg) > 0.0\n",
    "                )  # exp_avg has the same sign as exp_avg/denom\n",
    "\n",
    "                # decayed_exp_avg = torch.where(p.data.abs()>group['meta'], torch.zeros_like(p.data), exp_avg)\n",
    "\n",
    "                if p.dim() == 1:  # True if p is bias, false if p is weight\n",
    "                    p.data.addcdiv_(exp_avg, denom, value=-step_size)\n",
    "                else:\n",
    "                    decayed_exp_avg = torch.mul(\n",
    "                        torch.ones_like(p.data)\n",
    "                        - torch.pow(\n",
    "                            torch.tanh(group[\"meta\"][p.newname] * torch.abs(p.data)), 2\n",
    "                        ),\n",
    "                        exp_avg,\n",
    "                    )\n",
    "                    # p.data.addcdiv_(-step_size, exp_avg , denom)  #normal update\n",
    "                    p.data.addcdiv_(\n",
    "                        torch.where(condition_consolidation, decayed_exp_avg, exp_avg),\n",
    "                        denom,\n",
    "                        value=-step_size,\n",
    "                    )  # assymetric lr for metaplasticity\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "class Adam_bk(torch.optim.Optimizer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        params,\n",
    "        lr=1e-3,\n",
    "        betas=(0.9, 0.999),\n",
    "        n_bk=1,\n",
    "        ratios=[0],\n",
    "        areas=[1],\n",
    "        meta=0.0,\n",
    "        feedback=0.0,\n",
    "        eps=1e-8,\n",
    "        weight_decay=0,\n",
    "        amsgrad=False,\n",
    "        path=\".\",\n",
    "    ):\n",
    "        if not 0.0 <= lr:\n",
    "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
    "        if not 0.0 <= eps:\n",
    "            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n",
    "        if not 0.0 <= betas[0] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n",
    "        if not 0.0 <= betas[1] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n",
    "        defaults = dict(\n",
    "            lr=lr,\n",
    "            betas=betas,\n",
    "            n_bk=n_bk,\n",
    "            ratios=ratios,\n",
    "            areas=areas,\n",
    "            meta=meta,\n",
    "            feedback=feedback,\n",
    "            eps=eps,\n",
    "            weight_decay=weight_decay,\n",
    "            amsgrad=amsgrad,\n",
    "            path=path,\n",
    "        )\n",
    "        super(Adam_bk, self).__init__(params, defaults)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super(Adam_bk, self).__setstate__(state)\n",
    "        for group in self.param_groups:\n",
    "            group.setdefault(\"amsgrad\", False)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"Performs a single optimization step.\n",
    "\n",
    "        Arguments:\n",
    "            closure (callable, optional): A closure that reevaluates the model\n",
    "                and returns the loss.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            n_bk = group[\"n_bk\"]\n",
    "            ratios = group[\"ratios\"]\n",
    "            areas = group[\"areas\"]\n",
    "            meta = group[\"meta\"]\n",
    "            feedback = group[\"feedback\"]\n",
    "            path = group[\"path\"]\n",
    "\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "\n",
    "                grad = p.grad.data\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError(\n",
    "                        \"Adam does not support sparse gradients, please consider SparseAdam instead\"\n",
    "                    )\n",
    "                amsgrad = group[\"amsgrad\"]\n",
    "\n",
    "                state = self.state[p]\n",
    "\n",
    "                # State initialization\n",
    "                if len(state) == 0:\n",
    "                    state[\"step\"] = 0\n",
    "                    # Exponential moving average of gradient values\n",
    "                    state[\"exp_avg\"] = torch.zeros_like(p.data)\n",
    "                    # Exponential moving average of squared gradient values\n",
    "                    state[\"exp_avg_sq\"] = torch.zeros_like(p.data)\n",
    "                    # Initializing beakers\n",
    "                    for bk_idx in range(n_bk + 1):\n",
    "                        if bk_idx == n_bk:  # create an additional beaker clamped at 0\n",
    "                            state[\"bk\" + str(bk_idx) + \"_t-1\"] = torch.zeros_like(p)\n",
    "                            state[\"bk\" + str(bk_idx) + \"_t\"] = torch.zeros_like(p)\n",
    "                        else:  # create other beakers at equilibrium\n",
    "                            state[\"bk\" + str(bk_idx) + \"_t-1\"] = torch.empty_like(\n",
    "                                p\n",
    "                            ).copy_(p)\n",
    "                            state[\"bk\" + str(bk_idx) + \"_t\"] = torch.empty_like(\n",
    "                                p\n",
    "                            ).copy_(p)\n",
    "\n",
    "                        state[\"bk\" + str(bk_idx) + \"_lvl\"] = []\n",
    "\n",
    "                    if amsgrad:\n",
    "                        # Maintains max of all exp. moving avg. of sq. grad. values\n",
    "                        state[\"max_exp_avg_sq\"] = torch.zeros_like(p.data)\n",
    "\n",
    "                exp_avg, exp_avg_sq = state[\"exp_avg\"], state[\"exp_avg_sq\"]\n",
    "\n",
    "                if amsgrad:\n",
    "                    max_exp_avg_sq = state[\"max_exp_avg_sq\"]\n",
    "                beta1, beta2 = group[\"betas\"]\n",
    "\n",
    "                state[\"step\"] += 1\n",
    "\n",
    "                if group[\"weight_decay\"] != 0:\n",
    "                    grad.add_(group[\"weight_decay\"], p.data)  # p.data\n",
    "\n",
    "                # Decay the first and second moment running average coefficient\n",
    "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
    "                if amsgrad:\n",
    "                    # Maintains the maximum of all 2nd moment running avg. till now\n",
    "                    torch.max(max_exp_avg_sq, exp_avg_sq, out=max_exp_avg_sq)\n",
    "                    # Use the max. for normalizing running avg. of gradient\n",
    "                    denom = max_exp_avg_sq.sqrt().add_(group[\"eps\"])\n",
    "                else:\n",
    "                    denom = exp_avg_sq.sqrt().add_(group[\"eps\"])\n",
    "\n",
    "                bias_correction1 = 1 - beta1 ** state[\"step\"]\n",
    "                bias_correction2 = 1 - beta2 ** state[\"step\"]\n",
    "\n",
    "                step_size = group[\"lr\"] * math.sqrt(bias_correction2) / bias_correction1\n",
    "\n",
    "                if p.dim() == 1:  # True if p is bias, false if p is weight\n",
    "                    p.data.addcdiv_(-step_size, exp_avg, denom)\n",
    "                else:\n",
    "                    # weight update\n",
    "                    p.data.addcdiv_(-step_size, exp_avg, denom)\n",
    "                    p.data.add_(\n",
    "                        (ratios[0] / areas[0]) * (state[\"bk1_t-1\"] - state[\"bk0_t-1\"])\n",
    "                    )\n",
    "                    p.data.add_(\n",
    "                        torch.where(\n",
    "                            (state[\"bk\" + str(n_bk - 1) + \"_t-1\"] - state[\"bk0_t-1\"])\n",
    "                            * state[\"bk\" + str(n_bk - 1) + \"_t-1\"].sign()\n",
    "                            > 0,\n",
    "                            feedback\n",
    "                            * (state[\"bk\" + str(n_bk - 1) + \"_t-1\"] - state[\"bk0_t-1\"]),\n",
    "                            torch.zeros_like(p.data),\n",
    "                        )\n",
    "                    )\n",
    "                    # Update of the beaker levels\n",
    "                    with torch.no_grad():\n",
    "                        for bk_idx in range(1, n_bk):\n",
    "                            # diffusion entre les bk dans les deux sens + metaplasticit√© sur le dernier\n",
    "                            if bk_idx == (n_bk - 1):\n",
    "                                condition = (\n",
    "                                    state[\"bk\" + str(bk_idx - 1) + \"_t-1\"]\n",
    "                                    - state[\"bk\" + str(bk_idx) + \"_t-1\"]\n",
    "                                ) * state[\"bk\" + str(bk_idx) + \"_t-1\"] < 0\n",
    "                                decayed_m = (\n",
    "                                    1\n",
    "                                    - torch.tanh(\n",
    "                                        meta[p.newname]\n",
    "                                        * state[\"bk\" + str(bk_idx) + \"_t-1\"]\n",
    "                                    )\n",
    "                                    ** 2\n",
    "                                )\n",
    "                                state[\"bk\" + str(bk_idx) + \"_t\"] = torch.where(\n",
    "                                    condition,\n",
    "                                    state[\"bk\" + str(bk_idx) + \"_t-1\"]\n",
    "                                    + (ratios[bk_idx - 1] / areas[bk_idx])\n",
    "                                    * decayed_m\n",
    "                                    * (\n",
    "                                        state[\"bk\" + str(bk_idx - 1) + \"_t-1\"]\n",
    "                                        - state[\"bk\" + str(bk_idx) + \"_t-1\"]\n",
    "                                    )\n",
    "                                    + (ratios[bk_idx] / areas[bk_idx])\n",
    "                                    * (\n",
    "                                        state[\"bk\" + str(bk_idx + 1) + \"_t-1\"]\n",
    "                                        - state[\"bk\" + str(bk_idx) + \"_t-1\"]\n",
    "                                    ),\n",
    "                                    state[\"bk\" + str(bk_idx) + \"_t-1\"]\n",
    "                                    + (ratios[bk_idx - 1] / areas[bk_idx])\n",
    "                                    * (\n",
    "                                        state[\"bk\" + str(bk_idx - 1) + \"_t-1\"]\n",
    "                                        - state[\"bk\" + str(bk_idx) + \"_t-1\"]\n",
    "                                    )\n",
    "                                    + (ratios[bk_idx] / areas[bk_idx])\n",
    "                                    * (\n",
    "                                        state[\"bk\" + str(bk_idx + 1) + \"_t-1\"]\n",
    "                                        - state[\"bk\" + str(bk_idx) + \"_t-1\"]\n",
    "                                    ),\n",
    "                                )\n",
    "                            else:\n",
    "                                state[\"bk\" + str(bk_idx) + \"_t\"] = (\n",
    "                                    state[\"bk\" + str(bk_idx) + \"_t-1\"]\n",
    "                                    + (ratios[bk_idx - 1] / areas[bk_idx])\n",
    "                                    * (\n",
    "                                        state[\"bk\" + str(bk_idx - 1) + \"_t-1\"]\n",
    "                                        - state[\"bk\" + str(bk_idx) + \"_t-1\"]\n",
    "                                    )\n",
    "                                    + (ratios[bk_idx] / areas[bk_idx])\n",
    "                                    * (\n",
    "                                        state[\"bk\" + str(bk_idx + 1) + \"_t-1\"]\n",
    "                                        - state[\"bk\" + str(bk_idx) + \"_t-1\"]\n",
    "                                    )\n",
    "                                )\n",
    "\n",
    "                # Plotting beaker levels and distributions\n",
    "                fig = plt.figure(figsize=(12, 9))\n",
    "                for bk_idx in range(n_bk):\n",
    "                    if bk_idx == 0:\n",
    "                        state[\"bk\" + str(bk_idx) + \"_t-1\"] = p.data\n",
    "                    else:\n",
    "                        state[\"bk\" + str(bk_idx) + \"_t-1\"] = state[\n",
    "                            \"bk\" + str(bk_idx) + \"_t\"\n",
    "                        ]\n",
    "\n",
    "                    if p.size() == torch.empty(4096, 4096).size():\n",
    "                        state[\"bk\" + str(bk_idx) + \"_lvl\"].append(\n",
    "                            state[\"bk\" + str(bk_idx) + \"_t-1\"][11, 100].detach().item()\n",
    "                        )\n",
    "                        if state[\"step\"] % 600 == 0:\n",
    "                            plt.plot(state[\"bk\" + str(bk_idx) + \"_lvl\"])\n",
    "                            fig.savefig(path + \"/trajectory.png\", fmt=\"png\", dpi=300)\n",
    "                plt.close()\n",
    "\n",
    "                if p.dim() != 1 and state[\"step\"] % 600 == 0:\n",
    "                    fig2 = plt.figure(figsize=(12, 9))\n",
    "                    for bk_idx in range(n_bk):\n",
    "                        plt.hist(\n",
    "                            state[\"bk\" + str(bk_idx) + \"_t-1\"]\n",
    "                            .detach()\n",
    "                            .cpu()\n",
    "                            .numpy()\n",
    "                            .flatten(),\n",
    "                            100,\n",
    "                            label=\"bk\" + str(bk_idx),\n",
    "                            alpha=0.5,\n",
    "                        )\n",
    "                    plt.legend()\n",
    "                    fig2.savefig(\n",
    "                        path\n",
    "                        + \"/bk_\"\n",
    "                        + str(bk_idx)\n",
    "                        + \"_\"\n",
    "                        + str(p.size(0))\n",
    "                        + \"-\"\n",
    "                        + str(p.size(1))\n",
    "                        + \"_task\"\n",
    "                        + str((state[\"step\"] // 48000) % 2)\n",
    "                        + \".png\",\n",
    "                        fmt=\"png\",\n",
    "                    )\n",
    "                    torch.save(\n",
    "                        state,\n",
    "                        path\n",
    "                        + \"/state_\"\n",
    "                        + str(p.size(0))\n",
    "                        + \"-\"\n",
    "                        + str(p.size(1))\n",
    "                        + \"_task\"\n",
    "                        + str((state[\"step\"] // 48000) % 2)\n",
    "                        + \".tar\",\n",
    "                    )\n",
    "                    plt.close()\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "def train(\n",
    "    model,\n",
    "    train_loader,\n",
    "    current_task_index,\n",
    "    optimizer,\n",
    "    device,\n",
    "    prev_cons=None,\n",
    "    prev_params=None,\n",
    "    path_integ=None,\n",
    "    criterion=torch.nn.CrossEntropyLoss(),\n",
    "):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for data, target in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        if ewc:\n",
    "            ewc_loss = EWC_loss(\n",
    "                model,\n",
    "                prev_cons,\n",
    "                prev_params,\n",
    "                current_task_index,\n",
    "                device,\n",
    "                ewc_lambda=ewc_lambda,\n",
    "            )\n",
    "            total_loss = loss + ewc_loss\n",
    "        elif si:\n",
    "            p_prev, p_old = prev_params\n",
    "            si_loss = SI_loss(model, prev_cons, p_prev, si_lambda)\n",
    "            total_loss = loss + si_loss\n",
    "        else:\n",
    "            total_loss = loss\n",
    "\n",
    "        total_loss.backward()\n",
    "\n",
    "        # This loop is for BNN parameters having 'org' attribute\n",
    "        for p in list(\n",
    "            model.parameters()\n",
    "        ):  # blocking weights with org value greater than a threshold by setting grad to 0\n",
    "            if hasattr(p, \"org\"):\n",
    "                p.data.copy_(p.org)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        if si:\n",
    "            update_W(model, path_integ, p_old)\n",
    "\n",
    "        # This loop is only for BNN parameters as they have 'org' attribute\n",
    "        for p in list(model.parameters()):  # updating the org attribute\n",
    "            if hasattr(p, \"org\"):\n",
    "                p.org.copy_(p.data)\n",
    "\n",
    "\n",
    "def test(\n",
    "    model,\n",
    "    test_loader,\n",
    "    device,\n",
    "    criterion=torch.nn.CrossEntropyLoss(reduction=\"sum\"),\n",
    "    verbose=False,\n",
    "):\n",
    "\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    for data, target in test_loader:\n",
    "        if torch.cuda.is_available():\n",
    "            data, target = data.to(device), target.to(device)\n",
    "        output = model(data)\n",
    "        test_loss += criterion(output, target).item()  # mean batch loss\n",
    "        pred = output.data.max(1, keepdim=True)[\n",
    "            1\n",
    "        ]  # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_acc = round(100.0 * float(correct) / len(test_loader.dataset), 2)\n",
    "\n",
    "    if verbose:\n",
    "        print(\n",
    "            \"Test accuracy: {}/{} ({:.2f}%)\".format(\n",
    "                correct, len(test_loader.dataset), test_acc\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return test_acc, test_loss\n",
    "\n",
    "\n",
    "def estimate_fisher(model, dataset, device, num=1000, empirical=True):\n",
    "    # Estimate the FI-matrix for num batches of size 1\n",
    "\n",
    "    loader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=1, shuffle=False, num_workers=1\n",
    "    )\n",
    "\n",
    "    est_fisher_info = {}\n",
    "    for n, p in model.named_parameters():\n",
    "        if p.requires_grad:\n",
    "            n = n.replace(\".\", \"__\")\n",
    "            est_fisher_info[n] = p.detach().clone().zero_()\n",
    "\n",
    "    model.eval()\n",
    "    for index, (x, y) in enumerate(loader):\n",
    "        # break from for-loop if max number of samples has been reached\n",
    "\n",
    "        if index >= num:\n",
    "            break\n",
    "        # run forward pass of model\n",
    "        x = x.to(device)\n",
    "        output = model(x)\n",
    "        if empirical:\n",
    "            # -use provided label to calculate loglikelihood --> \"empirical Fisher\":\n",
    "            label = torch.LongTensor([y]) if type(y) == int else y\n",
    "            label = label.to(device)\n",
    "        else:\n",
    "            # -use predicted label to calculate loglikelihood:\n",
    "            label = output.max(1)[1]\n",
    "        # calculate negative log-likelihood\n",
    "        negloglikelihood = torch.nn.functional.nll_loss(\n",
    "            torch.nn.functional.log_softmax(output, dim=1), label\n",
    "        )\n",
    "\n",
    "        # Calculate gradient of negative loglikelihood\n",
    "        model.zero_grad()\n",
    "        negloglikelihood.backward()\n",
    "\n",
    "        # Square gradients and keep running sum\n",
    "        for n, p in model.named_parameters():\n",
    "            if p.requires_grad:\n",
    "                n = n.replace(\".\", \"__\")\n",
    "                if p.grad is not None:\n",
    "                    est_fisher_info[n] += p.grad.detach() ** 2\n",
    "\n",
    "    est_fisher_info = {n: p / index for n, p in est_fisher_info.items()}\n",
    "\n",
    "    return est_fisher_info\n",
    "\n",
    "\n",
    "def EWC_loss(\n",
    "    model,\n",
    "    previous_tasks_fisher,\n",
    "    previous_tasks_parameters,\n",
    "    current_task_index,\n",
    "    device,\n",
    "    ewc_lambda=5000,\n",
    "):\n",
    "\n",
    "    if current_task_index == 0:  # no task to remember -> return 0\n",
    "        return torch.tensor(0.0).to(device)\n",
    "    else:\n",
    "        losses = []\n",
    "        for task_idx in range(\n",
    "            current_task_index\n",
    "        ):  # for all previous tasks and parameters\n",
    "            for n, p in model.named_parameters():\n",
    "                if (p.requires_grad) and (n.find(\"bn\") == -1):\n",
    "                    n = n.replace(\".\", \"__\")\n",
    "                    mean = previous_tasks_parameters[n][task_idx]\n",
    "                    fisher = previous_tasks_fisher[n][task_idx]\n",
    "                    # print('in ewc loss, param =', p[0,0])\n",
    "                    losses.append((fisher * (p - mean) ** 2).sum())\n",
    "        return ewc_lambda * (1.0 / 2) * sum(losses)\n",
    "\n",
    "\n",
    "def update_omega(model, omega, p_prev, W, epsilon=0.1):\n",
    "    for n, p in model.named_parameters():\n",
    "        if n.find(\"bn\") == -1:  # not batchnorm\n",
    "            if p.requires_grad:\n",
    "                n = n.replace(\".\", \"__\")\n",
    "                if isinstance(model, BNN):\n",
    "                    p_current = p.org.detach().clone()  # sign()\n",
    "                else:\n",
    "                    p_current = p.detach().clone()\n",
    "                p_change = p_current - p_prev[n]\n",
    "                omega_add = W[n] / (p_change**2 + epsilon)\n",
    "                omega[n] += omega_add\n",
    "                print(\"parameter :\\t\", n, \"\\nomega :\\t\", omega[n])\n",
    "                W[n] = p.data.clone().zero_()\n",
    "    return omega\n",
    "\n",
    "\n",
    "def update_W(model, W, p_old):\n",
    "    for n, p in model.named_parameters():\n",
    "        if p.requires_grad and (n.find(\"bn\") == -1):\n",
    "            n = n.replace(\".\", \"__\")\n",
    "            if p.grad is not None:\n",
    "                if isinstance(model, BNN):\n",
    "                    if bin_path:\n",
    "                        W[n].add_(-p.grad * (p.sign().detach() - p_old[n]))\n",
    "                    else:\n",
    "                        W[n].add_(-p.grad * (p.org.detach() - p_old[n]))\n",
    "                else:\n",
    "                    W[n].add_(-p.grad * (p.detach() - p_old[n]))\n",
    "            if isinstance(model, BNN):\n",
    "                if bin_path:\n",
    "                    p_old[n] = p.sign().detach().clone()\n",
    "                else:\n",
    "                    p_old[n] = p.org.detach().clone()\n",
    "            else:\n",
    "                p_old[n] = p.detach().clone()\n",
    "\n",
    "\n",
    "def SI_loss(model, omega, prev_params, si_lambda):\n",
    "    losses = []\n",
    "    for n, p in model.named_parameters():\n",
    "        if p.requires_grad and (n.find(\"bn\") == -1):\n",
    "            n = n.replace(\".\", \"__\")\n",
    "            if isinstance(model, BNN):\n",
    "                losses.append(\n",
    "                    (omega[n] * (p - prev_params[n].sign()) ** 2).sum()\n",
    "                )  # org or sign\n",
    "                print(\"p =\\t\", p, \"\\np_prev =\\t\", prev_params[n])\n",
    "            else:\n",
    "                losses.append((omega[n] * (p - prev_params[n]) ** 2).sum())\n",
    "    return si_lambda * sum(losses)\n",
    "\n",
    "\n",
    "def switch_sign_induced_loss_increase(\n",
    "    model, loader, bins=10, sample=100, layer=2, num_run=1, verbose=False\n",
    "):\n",
    "    \"\"\"\n",
    "    The hidden weights of a given layer are split into bins of increasing magnitudes.\n",
    "    This function computes the increase in the loss produced by switching the sign of sample binary weights within each bins.\n",
    "    The choice of weights to be switched is made num_run times\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()  # model to evaluation mode\n",
    "    criterion = torch.nn.CrossEntropyLoss(reduction=\"none\")  # crossentropy loss\n",
    "    mbs = loader.batch_size\n",
    "\n",
    "    # initial magnitudes of the layer of the model and maximum magnitude\n",
    "    initial_weights = torch.empty_like(\n",
    "        model.layers[\"fc\" + str(layer)].weight.org\n",
    "    ).copy_(model.layers[\"fc\" + str(layer)].weight.org)\n",
    "    max_magnitude = initial_weights.abs().max().item()\n",
    "\n",
    "    hidden_value_total = torch.zeros((bins, num_run, 1))\n",
    "    total_result = torch.zeros((bins, num_run, 1))\n",
    "    effective_bin_index = []\n",
    "    bins_total_candidates = []\n",
    "\n",
    "    # constructing the switch masks for every bin\n",
    "    for k in range(bins):\n",
    "\n",
    "        # initializing lists\n",
    "        hidden_value_run = []  # will contain mean magnitude of every run\n",
    "        switch_list = (\n",
    "            []\n",
    "        )  # will contain a switch mask of fixed number of weight and bin for every run\n",
    "\n",
    "        for run in range(num_run):\n",
    "            # selecting weight candidate for switching by absolute magnitude belonging to bin\n",
    "            switch_indices = torch.where(\n",
    "                (initial_weights.abs() > (k / bins) * max_magnitude)\n",
    "                * (initial_weights.abs() < ((k + 1) / bins) * max_magnitude),\n",
    "                -torch.ones_like(initial_weights),\n",
    "                torch.ones_like(initial_weights),\n",
    "            )\n",
    "            bin_total = (\n",
    "                -1 * switch_indices[switch_indices == -1].sum().item()\n",
    "            )  # total of candidates\n",
    "\n",
    "            if run == 0:\n",
    "                bins_total_candidates.append(bin_total)\n",
    "\n",
    "            if bin_total >= sample:  # only if number of candidates greater than sample\n",
    "\n",
    "                cutoff = torch.ones_like(switch_indices[switch_indices == -1])\n",
    "                cutoff[\n",
    "                    sample:\n",
    "                ] *= -1  # removing candidates after accepting sample candidates\n",
    "                permut = torch.randperm(\n",
    "                    cutoff.nelement()\n",
    "                )  # shuffling to have different candidates every runs\n",
    "                switch_indices[switch_indices == -1] *= cutoff[\n",
    "                    permut\n",
    "                ]  # mask with only sample candidates switch of bin k\n",
    "                switch_list.append(switch_indices)\n",
    "\n",
    "                effective_switch = (\n",
    "                    -1 * switch_indices[switch_indices == -1].sum().item()\n",
    "                )\n",
    "                assert (\n",
    "                    effective_switch == sample\n",
    "                )  # make sure the mask has exactly sample switches\n",
    "\n",
    "                mean_hidden_value = (\n",
    "                    initial_weights[switch_indices == -1].abs().sum().item() / sample\n",
    "                )\n",
    "                hidden_value_run.append(mean_hidden_value / max_magnitude)\n",
    "            else:  # rejecting bins with not enough candidates\n",
    "                pass\n",
    "\n",
    "        effective_run = len(hidden_value_run)\n",
    "        iter_per_epoch = int(len(loader.dataset) / mbs)\n",
    "\n",
    "        if effective_run > 0:  # in this case effective_run = num_run\n",
    "            effective_bin_index.append(k)\n",
    "            loss_total = torch.zeros(\n",
    "                (effective_run, len(loader.dataset))\n",
    "            )  # initializing result tensor\n",
    "            for idx, (data, target) in enumerate(loader):\n",
    "                if torch.cuda.is_available():\n",
    "                    data, target = data.cuda(), target.cuda()\n",
    "\n",
    "                output_initial = model(data)\n",
    "                loss_initial_batch = criterion(output_initial, target)  # batch loss\n",
    "                if (idx % (iter_per_epoch / 2)) == 0 and verbose:\n",
    "                    print(\"\\nloss_initial_batch =\", loss_initial_batch)\n",
    "\n",
    "                for r in range(effective_run):  # loop over runs at fixed bin and batch\n",
    "\n",
    "                    if idx == 0 and verbose:\n",
    "                        print(\"\\nbin =\", k, \" run =\", r)\n",
    "\n",
    "                    model.layers[\"fc\" + str(layer)].weight.org.mul_(switch_list[r])\n",
    "\n",
    "                    control = (\n",
    "                        model.layers[\"fc\" + str(layer)].weight.org - initial_weights\n",
    "                    )\n",
    "                    if idx == 0 and verbose:\n",
    "                        print(\n",
    "                            \"mean value of switched hidden weights (must represent the bin)=\",\n",
    "                            control.abs().sum().item() / (2 * sample),\n",
    "                        )\n",
    "\n",
    "                    output_switch = model(data)\n",
    "                    loss_switch_batch = criterion(output_switch, target)  # batch loss\n",
    "\n",
    "                    control = (\n",
    "                        model.layers[\"fc\" + str(layer)].weight - initial_weights.sign()\n",
    "                    )\n",
    "                    if idx == 0 and verbose:\n",
    "                        print(\n",
    "                            \"mean value of swtiched binary weight (must equal 1)=\",\n",
    "                            control.abs().sum().item() / (2 * sample),\n",
    "                        )\n",
    "                        print(\n",
    "                            \"delta_loss_batch =\",\n",
    "                            (loss_switch_batch - loss_initial_batch),\n",
    "                        )\n",
    "\n",
    "                    model.layers[\"fc\" + str(layer)].weight.org.mul_(switch_list[r])\n",
    "\n",
    "                    control = (\n",
    "                        model.layers[\"fc\" + str(layer)].weight.org - initial_weights\n",
    "                    )\n",
    "                    if idx == 0 and verbose:\n",
    "                        print(\n",
    "                            \"delta hidden after switch back (must be zero) =\",\n",
    "                            control.abs().sum().item() / (2 * sample),\n",
    "                        )\n",
    "\n",
    "                    loss_total[r, idx * mbs : (idx + 1) * mbs] = (\n",
    "                        (loss_switch_batch - loss_initial_batch) / sample\n",
    "                    ).detach()\n",
    "\n",
    "            hidden_value_total[k, :] = torch.tensor(hidden_value_run).view(\n",
    "                effective_run, 1\n",
    "            )\n",
    "            total_result[k, :, :] = loss_total.mean(dim=1).view(\n",
    "                effective_run, 1\n",
    "            )  # mean over training data\n",
    "\n",
    "    if verbose:\n",
    "        print(\"list of candidates per bin =\", bins_total_candidates)\n",
    "\n",
    "    date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    time = datetime.now().strftime(\"%H-%M-%S\")\n",
    "    path = \"results/\" + date\n",
    "\n",
    "    effective_bin_index = torch.tensor(effective_bin_index)\n",
    "    hidden_value_cat_loss_increase = torch.cat(\n",
    "        [\n",
    "            hidden_value_total[effective_bin_index, :, :],\n",
    "            total_result[effective_bin_index, :, :],\n",
    "        ],\n",
    "        dim=2,\n",
    "    )\n",
    "\n",
    "    if not (os.path.exists(path)):\n",
    "        os.makedirs(path)\n",
    "    torch.save(\n",
    "        hidden_value_cat_loss_increase,\n",
    "        path\n",
    "        + \"/\"\n",
    "        + time\n",
    "        + \"_switch_sign_induced_loss_increase_bins-\"\n",
    "        + str(len(effective_bin_index))\n",
    "        + \"_sample-\"\n",
    "        + str(sample)\n",
    "        + \"_layer-\"\n",
    "        + str(layer)\n",
    "        + \"_runs-\"\n",
    "        + str(num_run)\n",
    "        + \".pt\",\n",
    "    )\n",
    "\n",
    "    return hidden_value_cat_loss_increase"
   ],
   "metadata": {
    "cellView": "form",
    "id": "poO2atp2Mceo"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# main.py"
   ],
   "metadata": {
    "collapsed": false,
    "id": "5YPXvriXMceu"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Archi [784, 4096, 4096, 10]\n",
      "meta {'l1': 1.35, 'l2': 1.35, 'l3': 1.35}\n",
      "ConvBNN(\n",
      "  (layers): ModuleDict(\n",
      "    (cv1): BinarizeConv2d(1, 32, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), bias=False)\n",
      "    (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (cv2): BinarizeConv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2), bias=False)\n",
      "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (fc3): BinarizeLinear(in_features=4096, out_features=10, bias=False)\n",
      "    (bn3): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      ")\n",
      "=> Train on pMNIST1\n",
      "Epoch 1\n",
      "Test on task pMNIST1\n",
      "Test accuracy: 9222/10000 (92.22%)\n",
      "Test on task pMNIST2\n",
      "Test accuracy: 903/10000 (9.03%)\n",
      "Test on task pMNIST3\n",
      "Test accuracy: 1133/10000 (11.33%)\n",
      "Epoch 2\n",
      "Test on task pMNIST1\n",
      "Test accuracy: 9281/10000 (92.81%)\n",
      "Test on task pMNIST2\n",
      "Test accuracy: 1298/10000 (12.98%)\n",
      "Test on task pMNIST3\n",
      "Test accuracy: 943/10000 (9.43%)\n",
      "Epoch 3\n",
      "Test on task pMNIST1\n",
      "Test accuracy: 9294/10000 (92.94%)\n",
      "Test on task pMNIST2\n",
      "Test accuracy: 975/10000 (9.75%)\n",
      "Test on task pMNIST3\n",
      "Test accuracy: 826/10000 (8.26%)\n",
      "Epoch 4\n",
      "Test on task pMNIST1\n",
      "Test accuracy: 9395/10000 (93.95%)\n",
      "Test on task pMNIST2\n",
      "Test accuracy: 840/10000 (8.40%)\n",
      "Test on task pMNIST3\n",
      "Test accuracy: 845/10000 (8.45%)\n",
      "Epoch 5\n",
      "Test on task pMNIST1\n",
      "Test accuracy: 9421/10000 (94.21%)\n",
      "Test on task pMNIST2\n",
      "Test accuracy: 795/10000 (7.95%)\n",
      "Test on task pMNIST3\n",
      "Test accuracy: 866/10000 (8.66%)\n",
      "Epoch 6\n",
      "Test on task pMNIST1\n",
      "Test accuracy: 9418/10000 (94.18%)\n",
      "Test on task pMNIST2\n",
      "Test accuracy: 735/10000 (7.35%)\n",
      "Test on task pMNIST3\n",
      "Test accuracy: 1164/10000 (11.64%)\n",
      "Epoch 7\n",
      "Test on task pMNIST1\n",
      "Test accuracy: 9415/10000 (94.15%)\n",
      "Test on task pMNIST2\n",
      "Test accuracy: 854/10000 (8.54%)\n",
      "Test on task pMNIST3\n",
      "Test accuracy: 851/10000 (8.51%)\n",
      "Epoch 8\n"
     ]
    }
   ],
   "source": [
    "if seed is not None:\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "time = datetime.now().strftime(\"%H-%M-%S\")\n",
    "path = f\"results/{date}_{time}\"\n",
    "if not (os.path.exists(path)):\n",
    "    os.makedirs(path)\n",
    "\n",
    "train_loader_list = []\n",
    "test_loader_list = []\n",
    "dset_train_list = []\n",
    "task_names = []\n",
    "\n",
    "for idx, task in enumerate(task_sequence):\n",
    "    if task == \"MNIST\":\n",
    "        train_loader_list.append(mnist_train_loader)\n",
    "        test_loader_list.append(mnist_test_loader)\n",
    "        dset_train_list.append(mnist_dset_train)\n",
    "        task_names.append(task)\n",
    "    elif task == \"FMNIST\":\n",
    "        train_loader_list.append(fashion_mnist_train_loader)\n",
    "        test_loader_list.append(fashion_mnist_test_loader)\n",
    "        dset_train_list.append(fmnist_dset_train)\n",
    "        task_names.append(task)\n",
    "    elif task == \"pMNIST\":\n",
    "        train_loader, test_loader, dset_train = create_permuted_loaders(task[1:])\n",
    "        train_loader_list.append(train_loader)\n",
    "        test_loader_list.append(test_loader)\n",
    "        dset_train_list.append(dset_train)\n",
    "        task_names.append(task + str(idx + 1))\n",
    "\n",
    "\n",
    "print(\"Archi\", archi)\n",
    "if net == \"bnn\":\n",
    "    model = BNN(archi, init=init, width=init_width, norm=norm).to(device)\n",
    "elif net == \"bcnn\":\n",
    "    model = ConvBNN(init=init, width=init_width, norm=norm).to(device)\n",
    "\n",
    "meta = {}\n",
    "for n, p in model.named_parameters():\n",
    "    index = int(n[9])\n",
    "    p.newname = \"l\" + str(index)\n",
    "    if (\"fc\" in n) or (\"cv\" in n):\n",
    "        meta[p.newname] = m\n",
    "print(\"meta\", meta)\n",
    "\n",
    "print(model)\n",
    "\n",
    "previous_tasks_parameters = {}\n",
    "previous_tasks_fisher = {}\n",
    "\n",
    "# ewc parameters initialization\n",
    "if ewc:\n",
    "    for n, p in model.named_parameters():\n",
    "        if (\n",
    "            n.find(\"bn\") == -1\n",
    "        ):  # we dont store bn parameters as we allow task dependent bn\n",
    "            n = n.replace(\".\", \"__\")\n",
    "            previous_tasks_fisher[n] = []\n",
    "            previous_tasks_parameters[n] = []\n",
    "elif si:\n",
    "    W = {}\n",
    "    p_prev = {}\n",
    "    p_old = {}\n",
    "    omega = {}\n",
    "    for n, p in model.named_parameters():\n",
    "        if p.requires_grad:\n",
    "            n = n.replace(\".\", \"__\")\n",
    "            W[n] = p.data.clone().zero_()\n",
    "            omega[n] = p.data.clone().zero_()\n",
    "            if net == \"bnn\":\n",
    "                p_prev[n] = p.data.clone()  # or sign\n",
    "                if bin_path:\n",
    "                    p_old[n] = p.data.sign().clone()\n",
    "                else:\n",
    "                    p_old[n] = p.data.clone()\n",
    "# Data collect initialisation\n",
    "data = {}\n",
    "data[\"net\"] = net\n",
    "data[\"scenario\"] = scenario\n",
    "arch = \"\"\n",
    "if not (net == \"bcnn\"):\n",
    "    for i in range(model.hidden_layers):\n",
    "        arch = arch + \"-\" + str(model.layers_dims[i + 1])\n",
    "\n",
    "data[\"arch\"] = arch[1:]\n",
    "data[\"norm\"] = norm\n",
    "data[\"lr\"], data[\"meta\"], data[\"ewc\"], data[\"SI\"], data[\"task_order\"] = (\n",
    "    [],\n",
    "    [],\n",
    "    [],\n",
    "    [],\n",
    "    [],\n",
    ")\n",
    "data[\"tsk\"], data[\"epoch\"] = [], []\n",
    "# no train eval to go faster\n",
    "data[\"acc_tr\"], data[\"loss_tr\"] = [], []\n",
    "\n",
    "for i in range(len(test_loader_list)):\n",
    "    data[\"acc_test_tsk_\" + str(i + 1)], data[\"loss_test_tsk_\" + str(i + 1)] = [], []\n",
    "\n",
    "name = \"_\" + data[\"net\"] + \"_\" + data[\"arch\"] + \"_\"\n",
    "\n",
    "for t in range(len(task_names)):\n",
    "    name = name + task_names[t] + \"-\"\n",
    "\n",
    "bn_states = []\n",
    "\n",
    "lrs = [lr * (gamma ** (-i)) for i in range(len(train_loader_list))]\n",
    "\n",
    "if beaker:\n",
    "    optimizer = Adam_bk(\n",
    "        model.parameters(),\n",
    "        lr=lr,\n",
    "        n_bk=n_bk,\n",
    "        ratios=ratios,\n",
    "        areas=areas,\n",
    "        feedback=fb,\n",
    "        meta=meta,\n",
    "        weight_decay=decay,\n",
    "        path=path,\n",
    "    )\n",
    "if si:\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=decay)\n",
    "\n",
    "for task_idx, task in enumerate(train_loader_list):\n",
    "    print(\"=> Train on\", task_names[task_idx])\n",
    "    if not (beaker or si):\n",
    "        optimizer = Adam_meta(\n",
    "            model.parameters(), lr=lrs[task_idx], meta=meta, weight_decay=decay\n",
    "        )\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        print(\"Epoch\", epoch)\n",
    "\n",
    "        if ewc:\n",
    "            train(\n",
    "                model,\n",
    "                task,\n",
    "                task_idx,\n",
    "                optimizer,\n",
    "                device,\n",
    "                prev_cons=previous_tasks_fisher,\n",
    "                prev_params=previous_tasks_parameters,\n",
    "            )\n",
    "        elif si:\n",
    "            train(\n",
    "                model,\n",
    "                task,\n",
    "                task_idx,\n",
    "                optimizer,\n",
    "                device,\n",
    "                prev_cons=omega,\n",
    "                path_integ=W,\n",
    "                prev_params=(p_prev, p_old),\n",
    "            )\n",
    "        else:\n",
    "            train(model, task, task_idx, optimizer, device)\n",
    "\n",
    "        data[\"task_order\"].append(task_idx + 1)\n",
    "        data[\"tsk\"].append(task_names[task_idx])\n",
    "        data[\"epoch\"].append(epoch)\n",
    "        data[\"lr\"].append(optimizer.param_groups[0][\"lr\"])\n",
    "\n",
    "        # No train eval to go faster\n",
    "        # train_accuracy, train_loss = test(model, task, device, verbose=True)\n",
    "        # data['acc_tr'].append(train_accuracy)\n",
    "        # data['loss_tr'].append(train_loss)\n",
    "\n",
    "        data[\"meta\"].append(meta)\n",
    "        data[\"ewc\"].append(ewc_lambda)\n",
    "        data[\"SI\"].append(si_lambda)\n",
    "\n",
    "        current_bn_state = model.save_bn_states()\n",
    "\n",
    "        for other_task_idx, other_task in enumerate(test_loader_list):\n",
    "            print(\"Test on task\", task_names[other_task_idx])\n",
    "            if scenario == \"task\":\n",
    "                if other_task_idx >= task_idx:\n",
    "                    model.load_bn_states(current_bn_state)\n",
    "                    test_accuracy, test_loss = test(\n",
    "                        model, other_task, device, verbose=True\n",
    "                    )\n",
    "                else:\n",
    "                    model.load_bn_states(bn_states[other_task_idx])\n",
    "                    test_accuracy, test_loss = test(\n",
    "                        model, other_task, device, verbose=True\n",
    "                    )\n",
    "\n",
    "            elif scenario == \"domain\":\n",
    "                test_accuracy, test_loss = test(model, other_task, device, verbose=True)\n",
    "\n",
    "            data[\"acc_test_tsk_\" + str(other_task_idx + 1)].append(test_accuracy)\n",
    "            data[\"loss_test_tsk_\" + str(other_task_idx + 1)].append(test_loss)\n",
    "\n",
    "        model.load_bn_states(current_bn_state)\n",
    "\n",
    "    bn_states.append(current_bn_state)\n",
    "    if ewc:\n",
    "        fisher = estimate_fisher(\n",
    "            model, dset_train_list[task_idx], device, num=5000, empirical=True\n",
    "        )\n",
    "        for n, p in model.named_parameters():\n",
    "            if n.find(\"bn\") == -1:  # not batchnorm\n",
    "                n = n.replace(\".\", \"__\")\n",
    "\n",
    "                # random consolidation\n",
    "                if rnd_consolidation:\n",
    "                    idx = torch.randperm(fisher[n].nelement())\n",
    "                    previous_tasks_fisher[n].append(\n",
    "                        fisher[n].view(-1)[idx].view(fisher[n].size())\n",
    "                    )\n",
    "\n",
    "                # EWC consolidation, comment when using random consolidation\n",
    "                previous_tasks_fisher[n].append(fisher[n])\n",
    "                previous_tasks_parameters[n].append(p.detach().clone())\n",
    "\n",
    "    elif si:\n",
    "        omega = update_omega(model, omega, p_prev, W)\n",
    "        for n, p in model.named_parameters():\n",
    "            if n.find(\"bn\") == -1:  # not batchnorm\n",
    "                n = n.replace(\".\", \"__\")\n",
    "                if net == \"bnn\":\n",
    "                    p_prev[n] = p.org.detach().clone()  # or sign\n",
    "                else:\n",
    "                    p_prev[n] = p.detach().clone()\n",
    "\n",
    "time = datetime.now().strftime(\"%H-%M-%S\")\n",
    "df_data = pd.DataFrame(data)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fysfuQV0Mcev",
    "outputId": "21a19279-58a5-4072-c961-bc313d537176"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "for i in range(len(test_loader_list)):\n",
    "    plt.plot(\n",
    "        df_data[\"epoch\"],\n",
    "        df_data[\"acc_test_tsk_\" + str(i + 1)],\n",
    "        label=\"Task \" + str(i + 1),\n",
    "    )\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": []
  },
  "accelerator": "GPU",
  "gpuClass": "standard"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
